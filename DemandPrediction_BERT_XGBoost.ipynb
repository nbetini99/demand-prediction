{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOcEv0ak/SUdruiir7PaeEE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbetini99/demand-prediction/blob/main/DemandPrediction_BERT_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers xgboost scikit-learn matplotlib pandas numpy"
      ],
      "metadata": {
        "id": "r90TVSwmaqYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "'''This section imports necessary libraries for data manipulation, visualization, machine learning, and text processing.\n",
        "numpy is used for numerical operations.\n",
        "pandas is used for data handling and manipulation using DataFrames.\n",
        "matplotlib.pyplot is for creating visualizations.\n",
        "train_test_split (from sklearn) is used to split the dataset.\n",
        "mean_squared_error (from sklearn) is used to calculate the model's prediction error.\n",
        "xgboost is the machine learning library used for the prediction model.\n",
        "BertTokenizer and BertModel (from transformers) are used to generate text embeddings.\n",
        "torch is a deep learning library, here used for BERT's operations.'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    r2_score\n",
        ")\n",
        "import xgboost as xgb\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Data Preparation\n",
        "# -----------------------------\n",
        "'''The below code section  defines a Python dictionary called data containing information about different products, including their descriptions, historical sales, region, seasonality, and the actual demand for the next month.\n",
        "This dictionary is then converted into a pandas DataFrame called df, a tabular data structure that is easier to manipulate'''\n",
        "# -----------------------------\n",
        "\n",
        "# Creating a synthetic dataset\n",
        "data = {\n",
        "    'product_description': [\n",
        "        'HP 65W Slim AC Adapter',\n",
        "        'Canon CLI-281 Black Ink Cartridge',\n",
        "        'Apple USB-C Power Adapter',\n",
        "        'Samsung Galaxy Charger 25W',\n",
        "        'Logitech Wireless Mouse M510'\n",
        "    ],\n",
        "    'historical_sales': [150, 120, 200, 180, 160],\n",
        "    'region': [1, 2, 1, 3, 2],\n",
        "    'seasonality': [0.3, 0.2, 0.4, 0.5, 0.1],\n",
        "    'demand_next_month': [170, 110, 210, 190, 150]\n",
        "}\n",
        "\n",
        "# Converting the dictionary to a pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Text Embedding using BERT\n",
        "# Load BERT model and tokenizer\n",
        "# -----------------------------\n",
        "'''The section below utilizes a pre-trained language model called BERT (bert-base-uncased) to convert product descriptions into numerical vectors (embeddings).\n",
        "It first loads the BERT tokenizer (tokenizer) to break down text into tokens and the BERT model (model) itself.\n",
        "The get_bert_embedding function takes a text input, processes it with BERT, and returns its embedding.\n",
        "The code then applies this function to all product descriptions in the DataFrame, storing the resulting embeddings in embeddings and then converting it into another DataFrame called embedding_df.'''\n",
        "# -----------------------------\n",
        "\n",
        "# Loading the pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to generate BERT embeddings for a given text\n",
        "def get_bert_embedding(text):\n",
        "    \"\"\"\n",
        "    Generates BERT embedding for the input text.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Extracting the embedding of the [CLS] token\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "    return cls_embedding\n",
        "\n",
        "# Applying the embedding function to all product descriptions\n",
        "embeddings = np.array([get_bert_embedding(desc) for desc in df['product_description']])\n",
        "\n",
        "# Creating a DataFrame from the embeddings\n",
        "embedding_df = pd.DataFrame(embeddings)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Feature Engineering\n",
        "# -----------------------------\n",
        "'''Here, the code combines the generated BERT embeddings with other numerical features (historical sales, region, and seasonality) into a single DataFrame called features. These are the input variables for our model.\n",
        "The target variable is defined as the 'demand_next_month' column from the original DataFrame, representing what we want to predict.\n",
        "The data is then split into training and testing sets using train_test_split.\n",
        "X_train and y_train are used to train the model.\n",
        "X_test and y_test are used to evaluate the model's performance on unseen data.'''\n",
        "# -----------------------------\n",
        "\n",
        "# Combining embeddings with structured features\n",
        "features = pd.concat([\n",
        "    embedding_df,\n",
        "    df[['historical_sales', 'region', 'seasonality']]\n",
        "], axis=1)\n",
        "\n",
        "# Defining the target variable\n",
        "target = df['demand_next_month']\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Model Training\n",
        "# -----------------------------\n",
        "'''The section below utilizes a pre-trained language model called BERT (bert-base-uncased) to convert product descriptions into numerical vectors (embeddings).\n",
        "It first loads the BERT tokenizer (tokenizer) to break down text into tokens and the BERT model (model) itself.\n",
        "The get_bert_embedding function takes a text input, processes it with BERT, and returns its embedding.\n",
        "The code then applies this function to all product descriptions in the DataFrame, storing the resulting embeddings in embeddings and then converting it into another DataFrame called embedding_df.'''\n",
        "# -----------------------------\n",
        "\n",
        "# Initializing the XGBoost regressor\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "\n",
        "# Training the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Model Evaluation\n",
        "# -----------------------------\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculating evaluation metrics\n",
        "\n",
        "# Mean Absolute Error (MAE): Average of absolute differences between actual and predicted values\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Mean Squared Error (MSE): Average of squared differences between actual and predicted values\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Root Mean Squared Error (RMSE): Square root of MSE, provides error in original units\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# R-squared (R²): Proportion of variance explained by the model\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Adjusted R-squared: Adjusts R² for the number of predictors in the model\n",
        "n = X_test.shape[0]  # Number of observations\n",
        "p = X_test.shape[1]  # Number of predictors\n",
        "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1) if n > p + 1 else None\n",
        "\n",
        "# Mean Absolute Percentage Error (MAPE): Average of absolute percentage errors\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "# Displaying the evaluation metrics\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"R-squared (R²): {r2:.2f}\")\n",
        "if adjusted_r2 is not None:\n",
        "    print(f\"Adjusted R-squared: {adjusted_r2:.2f}\")\n",
        "else:\n",
        "    print(\"Adjusted R-squared: Not applicable (n <= p + 1)\")\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "\n",
        "# -----------------------------\n",
        "# Visualization\n",
        "# -----------------------------\n",
        "\n",
        "# Plotting Actual vs. Predicted Demand\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(y_test.values, label='Actual Demand', marker='o')\n",
        "plt.plot(y_pred, label='Predicted Demand', marker='x')\n",
        "plt.title('Actual vs. Predicted Demand')\n",
        "plt.xlabel('Sample Index')\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    r2_score\n",
        ")\n",
        "import xgboost as xgb\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# -----------------------------\n",
        "# Data Preparation\n",
        "# -----------------------------\n",
        "\n",
        "# Creating a synthetic dataset\n",
        "data = {\n",
        "    'product_description': [\n",
        "        'HP 65W Slim AC Adapter',\n",
        "        'Canon CLI-281 Black Ink Cartridge',\n",
        "        'Apple USB-C Power Adapter',\n",
        "        'Samsung Galaxy Charger 25W',\n",
        "        'Logitech Wireless Mouse M510'\n",
        "    ],\n",
        "    'historical_sales': [150, 120, 200, 180, 160],\n",
        "    'region': [1, 2, 1, 3, 2],\n",
        "    'seasonality': [0.3, 0.2, 0.4, 0.5, 0.1],\n",
        "    'demand_next_month': [170, 110, 210, 190, 150]\n",
        "}\n",
        "\n",
        "# Converting the dictionary to a pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# -----------------------------\n",
        "# Text Embedding using BERT\n",
        "# -----------------------------\n",
        "\n",
        "# Loading the pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to generate BERT embeddings for a given text\n",
        "def get_bert_embedding(text):\n",
        "    \"\"\"\n",
        "    Generates BERT embedding for the input text.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Extracting the embedding of the [CLS] token\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "    return cls_embedding\n",
        "\n",
        "# Applying the embedding function to all product descriptions\n",
        "embeddings = np.array([get_bert_embedding(desc) for desc in df['product_description']])\n",
        "\n",
        "# Creating a DataFrame from the embeddings\n",
        "embedding_df = pd.DataFrame(embeddings)\n",
        "\n",
        "# -----------------------------\n",
        "# Feature Engineering\n",
        "'''Here, the code combines the generated BERT embeddings with other numerical features (historical sales, region, and seasonality) into a single DataFrame called features. These are the input variables for our model.\n",
        "The target variable is defined as the 'demand_next_month' column from the original DataFrame, representing what we want to predict.\n",
        "The data is then split into training and testing sets using train_test_split.\n",
        "X_train and y_train are used to train the model.\n",
        "X_test and y_test are used to evaluate the model's performance on unseen data.'''\n",
        "# -----------------------------\n",
        "\n",
        "# Combining embeddings with structured features\n",
        "features = pd.concat([\n",
        "    embedding_df,\n",
        "    df[['historical_sales', 'region', 'seasonality']]\n",
        "], axis=1)\n",
        "\n",
        "# Defining the target variable\n",
        "target = df['demand_next_month']\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Model Training\n",
        "'''The section below utilizes a pre-trained language model called BERT (bert-base-uncased) to convert product descriptions into numerical vectors (embeddings).\n",
        "It first loads the BERT tokenizer (tokenizer) to break down text into tokens and the BERT model (model) itself.\n",
        "The get_bert_embedding function takes a text input, processes it with BERT, and returns its embedding.\n",
        "The code then applies this function to all product descriptions in the DataFrame, storing the resulting embeddings in embeddings and then converting it into another DataFrame called embedding_df.'''\n",
        "\n",
        "# -----------------------------\n",
        "\n",
        "# Initializing the XGBoost regressor\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "\n",
        "# Training the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Model Evaluation\n",
        "# -----------------------------\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculating evaluation metrics\n",
        "\n",
        "# Mean Absolute Error (MAE): Average of absolute differences between actual and predicted values\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Mean Squared Error (MSE): Average of squared differences between actual and predicted values\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Root Mean Squared Error (RMSE): Square root of MSE, provides error in original units\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# R-squared (R²): Proportion of variance explained by the model\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Adjusted R-squared: Adjusts R² for the number of predictors in the model\n",
        "n = X_test.shape[0]  # Number of observations\n",
        "p = X_test.shape[1]  # Number of predictors\n",
        "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1) if n > p + 1 else None\n",
        "\n",
        "# Mean Absolute Percentage Error (MAPE): Average of absolute percentage errors\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "# Displaying the evaluation metrics\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"R-squared (R²): {r2:.2f}\")\n",
        "if adjusted_r2 is not None:\n",
        "    print(f\"Adjusted R-squared: {adjusted_r2:.2f}\")\n",
        "else:\n",
        "    print(\"Adjusted R-squared: Not applicable (n <= p + 1)\")\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Visualization\n",
        "# -----------------------------\n",
        "\n",
        "# Plotting Actual vs. Predicted Demand\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(y_test.values, label='Actual Demand', marker='o')\n",
        "plt.plot(y_pred, label='Predicted Demand', marker='x')\n",
        "plt.title('Actual vs. Predicted Demand')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Demand')\n",
        "plt.legend() # Added parentheses to call the legend function\n",
        "plt.show() # Added plt.show() to display the plot"
      ],
      "metadata": {
        "id": "KD6VDu_LeCLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M6SRi4sakxC"
      },
      "outputs": [],
      "source": [
        "# Install these packages in Colab if not already installed\n",
        "'''This section imports necessary libraries for data manipulation, visualization, machine learning, and text processing.\n",
        "numpy is used for numerical operations.\n",
        "pandas is used for data handling and manipulation using DataFrames.\n",
        "matplotlib.pyplot is for creating visualizations.\n",
        "train_test_split (from sklearn) is used to split the dataset.\n",
        "mean_squared_error (from sklearn) is used to calculate the model's prediction error.\n",
        "xgboost is the machine learning library used for the prediction model.\n",
        "BertTokenizer and BertModel (from transformers) are used to generate text embeddings.\n",
        "torch is a deep learning library, here used for BERT's operations.'''\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "\n",
        "# Sample Data and Data preparation\n",
        "\n",
        "'''The below code section  defines a Python dictionary called data containing information about different products, including their descriptions, historical sales, region, seasonality, and the actual demand for the next month.\n",
        "This dictionary is then converted into a pandas DataFrame called df, a tabular data structure that is easier to manipulate'''\n",
        "\n",
        "data = {\n",
        "    'product_description': [\n",
        "        'XYZ Smart Thermostat ',\n",
        "        'ABC Condensor Cooler',\n",
        "        'PQR Air Conditioner',\n",
        "        'Heat furnace ABC',\n",
        "        'Zoning HVAC System',\n",
        "        'Air Quality sensor',\n",
        "        'Humidity Sensor'\n",
        "    ],\n",
        "    'historical_sales': [150, 120, 200, 180, 160, 133, 189],\n",
        "    'region': [1, 2, 1, 3, 2, 2, 3],\n",
        "    'seasonality': [0.3, 0.2, 0.4, 0.5, 0.1, 0.23, 0.42],\n",
        "    'demand_next_month': [120, 144, 170, 110, 210, 190, 150]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "# Text Embedding with BERT\n",
        "'''The section below utilizes a pre-trained language model called BERT (bert-base-uncased) to convert product descriptions into numerical vectors (embeddings).\n",
        "It first loads the BERT tokenizer (tokenizer) to break down text into tokens and the BERT model (model) itself.\n",
        "The get_bert_embedding function takes a text input, processes it with BERT, and returns its embedding.\n",
        "The code then applies this function to all product descriptions in the DataFrame, storing the resulting embeddings in embeddings and then converting it into another DataFrame called embedding_df.'''\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = np.array([get_bert_embedding(desc) for desc in df['product_description']])\n",
        "embedding_df = pd.DataFrame(embeddings)\n",
        "\n",
        "# Combine with tabular features\n",
        "# Feature Engineering and Data Splitting\n",
        "features = pd.concat([embedding_df, df[['historical_sales', 'region', 'seasonality']]], axis=1)\n",
        "target = df['demand_next_month']\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Training and Prediction\n",
        "# Train XGBoost\n",
        "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Visualization and Evaluation\n",
        "# # Predict and visualize\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(y_test.values, label='Actual', marker='o')\n",
        "plt.plot(y_pred, label='Predicted', marker='x')\n",
        "plt.title('Actual vs Predicted Demand')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Demand')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print MSE\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
      ]
    }
  ]
}